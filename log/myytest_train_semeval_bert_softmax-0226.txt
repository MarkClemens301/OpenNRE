D:\ProgramData\Anaconda3\envs\tf\python.exe D:/Public/Documents/GitHub/OpenNRE/myytest_train_semeval_bert_softmax.py
True
2021-02-26 01:43:40,615 - root - INFO - Loading BERT pre-trained checkpoint.
2021-02-26 01:43:40,616 - transformers.configuration_utils - INFO - loading configuration file pretrain/bert-base-uncased\config.json
2021-02-26 01:43:40,616 - transformers.configuration_utils - INFO - Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-26 01:43:40,616 - transformers.modeling_utils - INFO - loading weights file pretrain/bert-base-uncased\pytorch_model.bin
2021-02-26 01:43:41,892 - transformers.modeling_utils - WARNING - Some weights of the model checkpoint at pretrain/bert-base-uncased were not used when initializing BertModel: ['bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2021-02-26 01:43:41,897 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at pretrain/bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
2021-02-26 01:43:41,897 - transformers.tokenization_utils_base - INFO - Model name 'pretrain/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'pretrain/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-26 01:43:41,898 - transformers.tokenization_utils_base - INFO - Didn't find file pretrain/bert-base-uncased\added_tokens.json. We won't load it.
2021-02-26 01:43:41,898 - transformers.tokenization_utils_base - INFO - Didn't find file pretrain/bert-base-uncased\special_tokens_map.json. We won't load it.
2021-02-26 01:43:41,898 - transformers.tokenization_utils_base - INFO - Didn't find file pretrain/bert-base-uncased\tokenizer_config.json. We won't load it.
2021-02-26 01:43:41,899 - transformers.tokenization_utils_base - INFO - Didn't find file pretrain/bert-base-uncased\tokenizer.json. We won't load it.
2021-02-26 01:43:41,899 - transformers.tokenization_utils_base - INFO - loading file pretrain/bert-base-uncased\vocab.txt
2021-02-26 01:43:41,899 - transformers.tokenization_utils_base - INFO - loading file None
2021-02-26 01:43:41,899 - transformers.tokenization_utils_base - INFO - loading file None
2021-02-26 01:43:41,899 - transformers.tokenization_utils_base - INFO - loading file None
2021-02-26 01:43:41,899 - transformers.tokenization_utils_base - INFO - loading file None
2021-02-26 01:43:41,984 - root - INFO - Loaded sentence RE dataset benchmark/test_data/semeval_train.txt with 106 lines and 2 relations.
2021-02-26 01:43:41,985 - root - INFO - Loaded sentence RE dataset benchmark/test_data/semeval_val.txt with 15 lines and 2 relations.
2021-02-26 01:43:41,988 - root - INFO - Loaded sentence RE dataset benchmark/test_data/semeval_test.txt with 31 lines and 2 relations.
2021-02-26 01:43:43,916 - root - INFO - === Epoch 0 train ===
100%|██████████| 14/14 [00:07<00:00,  1.81it/s, acc=0.446, loss=0.748]
2021-02-26 01:43:51,635 - root - INFO - === Epoch 0 val ===
100%|██████████| 2/2 [00:00<00:00,  5.25it/s, acc=0.4]
2021-02-26 01:43:52,017 - root - INFO - Evaluation result: {'acc': 0.4, 'micro_p': 0.4, 'micro_r': 1.0, 'micro_f1': 0.5714285714285715}.
2021-02-26 01:43:52,017 - root - INFO - Metric micro_f1 current / best: 0.5714285714285715 / 0
2021-02-26 01:43:52,017 - root - INFO - Best ckpt and saved.
2021-02-26 01:43:53,629 - root - INFO - === Epoch 1 train ===
100%|██████████| 14/14 [00:07<00:00,  1.88it/s, acc=0.482, loss=0.762]
2021-02-26 01:44:01,077 - root - INFO - === Epoch 1 val ===
100%|██████████| 2/2 [00:00<00:00,  5.25it/s, acc=0.467]
2021-02-26 01:44:01,459 - root - INFO - Evaluation result: {'acc': 0.4666666666666667, 'micro_p': 0.4166666666666667, 'micro_r': 0.8333333333333334, 'micro_f1': 0.5555555555555556}.
2021-02-26 01:44:01,460 - root - INFO - Metric micro_f1 current / best: 0.5555555555555556 / 0.5714285714285715
2021-02-26 01:44:01,460 - root - INFO - Best micro_f1 on val set: 0.571429
100%|██████████| 4/4 [00:00<00:00,  7.61it/s, acc=0.387]
Accuracy on test set: 0.3870967741935484
Micro Precision: 0.3870967741935484
Micro Recall: 1.0
Micro F1: 0.5581395348837209
2021-02-26 01:44:02,278 - root - INFO - Evaluation result: {'acc': 0.3870967741935484, 'micro_p': 0.3870967741935484, 'micro_r': 1.0, 'micro_f1': 0.5581395348837209}.

Process finished with exit code 0
